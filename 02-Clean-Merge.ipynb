{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2: Cleaning and Merging\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Getting Started](#Getting-Started)\n",
    "* [Loading, Merging, and Cleaning Similar Data](#Loading,-Merging,-and-Cleaning-Similar-Data)\n",
    "    * [Loading a CSV: Franklin County Audit Data](#Loading-a-CSV:-Franklin-County-Audit-Data)\n",
    "    * [Parsing Errors: Licking County Auditor Data](#Parsing-Errors:-Licking-County-Auditor-Data)\n",
    "    * [Extraction from a GIS Dataset: Fairfield County Auditor Data](#Extraction-from-a-GIS-Dataset:-Fairfield-County-Auditor-Data)\n",
    "* [Joining Related Datasets](#Joining-Related-Datasets)\n",
    "* [Lab Answers](#Lab-Answers)\n",
    "* [Next Steps](#Next-Steps)\n",
    "* [Resources and Further Reading](#Resources-and-Further-Reading)\n",
    "* [Exercises](#Exercises)\n",
    "\n",
    "### Lab Questions\n",
    "\n",
    "[1](#Lab-1), [2](#Lab-2), [3](#Lab-3), [4](#Lab-4), [5](#Lab-5), [6](#Lab-6), [7](#Lab-7), [8](#Lab-8), [9](#Lab-9),  [10](#Lab-10), [11](#Lab-11), [12](#Lab-12), [13](#Lab-13), [14](#Lab-14), [15](#Lab-15)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In addition to libraries we used in the last unit, this notebook relies on the [GeoPandas](library) to process data from a [geographic information system](https://en.wikipedia.org/wiki/Geographic_information_system).  To install the library, we can use `pip`.\n",
    "\n",
    "If using [Anaconda](https://www.anaconda.com/download) and the following pip command fails, open the Anaconda prompt on your computer and run the following\n",
    "\n",
    "```\n",
    "conda install --yes geopandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with county auditor data containing real estate information.  To reduce the memory and time required to process the data, the size of one of the datasets was reduced; the *sample_file()* function below contains the code to do this. Two arguments are required when the function is called, *input_file* and *output_file*, to specify the source and target files, respectively.  An keyword argument, *fraction*, can be specified to set the desired size of the output file relative to the input file; the default value is 0.1.  \n",
    "\n",
    "First, *sample_file()* calls *get_line_count()* to calculate the total number of lines in the source file. The total is multiplied by the specified fraction and truncated to the nearest integer to determine the number of lines that should be in the output file.  Next, the [*sample()*](https://docs.python.org/3/library/random.html#random.sample) function from the random module is used to select a sampling of line numbers from a range of values starting at 1 and ending at the last line number in the source file; the number of line numbers in the sample is equal to the calculated sample line count.  Finally, the function iterates through the source file, line-by-line, and copies a line to the output file if that line's line number is in the sample of line numbers.\n",
    "\n",
    "The *sample_file()* function was used to reduce the Franklin County Auditor data from over 400,000 lines to about 40,000 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import random\n",
    "\n",
    "def get_line_count(input_file):\n",
    "    \"\"\"Count number of lines in a file\"\"\"\n",
    "    count = 0\n",
    "    with open(input_file) as infile:\n",
    "        for line in infile:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def sample_file(input_file, output_file, fraction=0.1):\n",
    "    \"\"\"Exctract a subset of lines from a file\"\"\"\n",
    "    total_line_count = get_line_count(input_file)\n",
    "    sample_line_count = int(fraction * total_line_count)  # fraction of total\n",
    "    sample_line_numbers = random.sample(range(1, total_line_count), \n",
    "                                        sample_line_count)  # sample of line numbers\n",
    "    sample_line_numbers.sort()\n",
    "    sample_line_numbers.insert(0, 0)\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for line_number in sample_line_numbers:\n",
    "            line = linecache.getline(input_file, line_number + 1)\n",
    "            outfile.write(line)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Merging, and Cleaning Similar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the analysis we would like to complete requires gather data from multiple sources.  Working with multiple sources can present some challenges that should be addressed before analyzing the data.  Rather than keeping source data separate, its usually convenient to store similar data together - combining separate sources into one database table to similar data store. Combining sources require extracting relevant data and reorganizing it to fit the target structure.  \n",
    "\n",
    "In addition to simply extracting the data, we often need to address data quality issues as well; examples of quality issues include\n",
    "\n",
    "- *duplication*: the dataset unnecessarily includes repeated data\n",
    "- *inconsistency*: different values are used to represent the same thing or the values do not fit the defined schema\n",
    "- *incompleteness*: data is missing from the dataset\n",
    "- *inaccuracy*: the data does not reflect what it purports to measure or represent\n",
    "\n",
    "Resolving duplication issues is usually straightforward: duplicate data is removed before conducting further analysis. Inconsistencies can be resolved by determining the appropriate values and transforming the data as needed; however, realizing that multiple values correspond to the same thing might require some examination of the data.  While it can be easy to detect incompleteness of data, the approach for resolving the issue might be context-specific. Should a default value be used? Should a randomly generated value within some range be substituted? Should records with missing data be dropped entirely? Inaccuracies tend to be more difficult to detect and to resolve as they often require examining how the source data was collected.  \n",
    "\n",
    "In the following examples, we'll work on aggregating data from three different county auditor datasets. Our immediate objective is to collect any data regarding appraisal values, sale price, total area, the number of rooms, information about heating and cooling, and the year built for residential properties. Later, we'll use this data to determine if there's a relationship between price or appraisal value and the other properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a CSV: Franklin County Audit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first data source from which we'll extract data is a CSV containing [data from the Franklin County Auditor](ftp://apps.franklincountyauditor.com/). As noted above, the data as been sampled to reduce its size from 400,000 records to 40,000.  The Auditor's site provides documentation of the dataset.  Though the documentation appears to be outdated, it does provide some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display auditor documentation in the notebook\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"./data/02-franklin-description.pdf\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the panda's *read_csv()* method to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "franklin = pd.read_csv('./data/02-franklin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, we can now begin examining it. To start, we can see the complete list of columns in the dataset.  Compare this to the columns in the documentation - there are column names in the dataset that do not appear in the documentation and column names in the documentation that do not appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the first few rows using the DataFrame's *head()* method.  We'll increase the number of columns displayed to 200 to accommodate the datasets we'll be working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', 200)\n",
    "franklin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining these rows, we can get a sense of the type of data in each column.  We can also see that some values are `NaN` which stands for \"Not a Number\" and is used when no value is present, i.e. when data is missing.  We'll address these values later.\n",
    "\n",
    "As noted earlier, we'd like to extract appraisal value, sale price, and other data for residential real estate.  Based on the documentation, the `PCLASS` field should indicate a parcel's property class.  The first few rows of data are consistent with the documentation.  To see all the values that appear in the `PCLASS` field, we can use the *unique()* method for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin.PCLASS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the number of records with each value using the *value_counts()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin.PCLASS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation indicates that the `PROPTYP` also includes property type information; however, the first five rows do not have values for this field.  \n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-1\"></a><mark> **Lab 1** Using *unique()*  or *value_counts()* in the cell below, confirm that none of the rows have data for the `PROPTYP` field.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "In a future unit, we'll explore how price or appraisal value is dependent on other factors such as number of bathrooms or the year in which a building a was built. In order to do this analysis, we'll need to extract the relevant data. \n",
    "\n",
    "Based on the documentation and the first few rows of data, we might be interested in extracting the following columns from the larger dataset.\n",
    "\n",
    "- `APPRLND`: appraisal land value\n",
    "- `APPRBLD`: appraisal building value\n",
    "- `PCLASS`: property class\n",
    "- `PRICE`: sales price\n",
    "- `AREA_A`: building area\n",
    "- `ROOMS`: total number of rooms\n",
    "- `BATHS`: number of full bathrooms\n",
    "- `HBATHS`: number of half bathrooms\n",
    "- `BEDRMS`: number of bedrooms\n",
    "- `AIRCOND`: heating and/or air conditioning\n",
    "- `FIREPLC`: presence of a fireplace\n",
    "- `YEARBLT`: the build year\n",
    "\n",
    "To extract these columns, we'll first create a list containing their names then create a copy of the DataFrame consisting only of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home properties or fields we can use to filter out data\n",
    "franklin_columns = ['APPRLND', 'APPRBLD', 'PCLASS', 'PRICE', 'AREA_A', 'ROOMS','BATHS', \n",
    "                    'HBATHS', 'BEDRMS', 'AIRCOND', 'FIREPLC','YEARBLT']\n",
    "\n",
    "# copy vs view\n",
    "franklin_subset = franklin[franklin_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in `franklin_subset` is a copy of the source data.  We can manipulate the copy while leaving the full dataset unchanged.  This can be helpful if we make a mistake or need to see what a value might have been prior to manipulation. Alternatively, we could just reload the data whenever necessary.\n",
    "\n",
    "The first thing we can do is remove data for non-residential properties.  As shown above, 37,902 records correspond to residential properties.  To filter the data, we can use a mask and bracket notation with the DataFrame.  The mask we'll need is one that evaluates to `True` when the value of `PCLASS` is `R`.\n",
    "\n",
    "One we've filtered the data, we can drop the `PCLASS` column as it is no longer needed.  To do this, we'll use the DataFrame's *drop()* method and specify the column name and axis.  We'll specify an axis value of *1* to indicate that we'd like to drop a column as opposed to a value of *0* to drop a row. We'll also use the *inplace* keyword argument to indicate that we'd like to manipulate the DataFrame itself rather than to return a DataFrame with the dropped column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data using a mask\n",
    "franklin_subset = franklin_subset[franklin_subset.PCLASS == 'R']\n",
    "\n",
    "# drop the PCLASS column\n",
    "franklin_subset.drop(['PCLASS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that that the DataFrame has been filtered by comparing number of records in the `franklin_subset` DataFrame to the number of records in the `franklin` DataFrame.\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-2\"></a><mark> **Lab 2** In the cell below, use *len()* and a comparison operator to confirm that the number of records in the `franklin_subset` DataFrame is less than the number of records in the `franklin` DataFrames.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Let's look at the the appraisal-related fields, `APPRLND` and `APPRBLD`.  From above, we know that that data in the `APPRBLD` field is stored as floating point numbers. We can use the *describe()* method to calculate some descriptive statistics for `APPRBLD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.APPRBLD.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the minimum value is zero.  Let's see how many records have a building appraisal value of zero. Because the data is stored as floating point numbers, we should be aware of the [issues](https://docs.python.org/3/tutorial/floatingpoint.html) related to floating point values.  If we choose to continue working with the data as floating point values, we can use the [NumPy *isclose()*](https://docs.scipy.org/doc/numpy/reference/generated/numpy.isclose.html) function to create a mask to compare values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(franklin_subset[pd.np.isclose(franklin_subset.APPRBLD, 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to working with floating point values, we can convert a column's datatype to `int` when appropriate.  Here, an integer would represent whole dollar amounts and would be meaningful; if decimals are used to record fractions of a dollar, we won't loose much information.  As a Series, each column has an *astype()* method that can be used to convert the column's type.  The method creates a copy so we have to reassign the DataFrame's column when doing the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset['APPRBLD'] = franklin_subset.APPRBLD.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is stored as integers, we can make comparisons more directly using the standard operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(franklin_subset[franklin_subset.APPRBLD == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the data to include only the rows where the building appraisal value is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset = franklin_subset[franklin_subset.APPRBLD > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a name=\"Lab-3\"></a><mark> **Lab 3** In the cell below, filter the `franklin_subset` DataFrame to exclude rows that have a `APPRLND` of zero.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify comparisons and other analysis later, we might choose to combine the data related to number of bathrooms into on column.  Because the data currently distinguishes between full and half baths, we can calculate the total number of bathrooms as the sum of the value of `BATH` and half the value of `HBATHS`.  Note that this has the effect of counting two half-bathrooms as a full bathroom; while two half-bathrooms might effect price differently than a full bathroom, we'll effectively ignore any such effect.\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-4\"></a><mark> **Lab 4** The data type of the `HBATH` and `HBATHS` should a numeric type (an integer or a floating point value) in order to calculate the combined value directly from the existing values.  In the cell below, use the `dtypes` property to confirm that the `HBATH` and `HBATHS` columns have a numeric data type.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Rather than using a for loop and calculating net number of bathrooms for each row, pandas supports element-wise multiplication and addition allowing use to do the following.  For this calculation we will treat missing data in the same was as a zero value.  To do this, we use the *fillna()* method for the appropriate column and specify the value we'd like to use in place of missing values - zero, in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset[\"Bathrooms\"] = franklin_subset.BATHS.fillna(0) + 0.5 * franklin_subset.HBATHS.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the number of bathrooms as defined above and creates stores each row's value in a new column named `Bathrooms`.  \n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-5\"></a><mark> **Lab 5** We no longer need the `BATHS` or `HBATHS` columns. In the cell below, use the *drop()* method to remove these columns from the `franklin_subset` DataFrame.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Next, let's look at the `AIRCOND` column.  The documentation indicates that the field can take one of three values:\n",
    "\n",
    "- 0: No heating or air conditioning\n",
    "- 1: Heat\n",
    "- 2: Air conditioning and heat\n",
    "\n",
    "Let's compare this to the actual values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values in aircond\n",
    "franklin_subset.AIRCOND.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `AIRCOND` column doesn't contain any records with a value of 2.  At this point, we might contact the person responsible for maintaining the data for clarification.  For our work, we'll that all residential parcels in the dataset have heat and `AIRCOND` here indicates whether or not air conditioning is available.  We can change the data type of the column to reflect this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.AIRCOND = franklin_subset.AIRCOND.astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to to the `FIREPLC` column, we can list the unique values to see that the dataset is again inconsistent with the documentation; rather than containing a single character representing the presence or absence of a fireplace the dataset instead contains integer values that likely indicate the number of fireplaces installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.FIREPLC.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `nan` is among the values.  \n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-6\"></a><mark> **Lab 6** In the cell below, use the *fillna* property with the `FIREPLC` column to replace missing values with zero.  Either reassign the DataFrame's `FIREPLC` column with modified data or specify `inplace=True` as an argument to *fillna()* to alter the column in-place.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "At this point we have the following columns and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the next dataset, it might be useful to give the columns more descriptive names.  First, let's create a copy of the DataFrame in case we need access to the data in its current state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data = franklin_subset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rename the columns, we can use the DataFrame's *rename()* method. When calling the method, we can pass a dictionary that maps the existing column names to new names. We'll also specify that we want to change column names rather than index labels by specifying `axis=1` and that we'd like to alter the DataFrame itself rather than return a copy with the alteration using `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.rename(\n",
    "    {'APPRLND': 'AppraisedLand',\n",
    "     'APPRBLD': 'AppraisedBuilding',\n",
    "     'PRICE': 'SalePrice',\n",
    "     'AREA_A': 'Area', \n",
    "     'ROOMS':'Rooms',\n",
    "     'BEDRMS':'Bedrooms',\n",
    "     'AIRCOND': 'AirConditioning', \n",
    "     'FIREPLC': 'Fireplaces', \n",
    "     'YEARBLT': 'YearBuilt'\n",
    "    },\n",
    "    axis=1 ,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a name=\"Lab-7\"></a><mark> **Lab 7** In the cell below, verify that the columns of the `home_data` DataFrame have been changed.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "As noted above, we'd also like to record whether or not a parcel includes heating.  Earlier we assumed that all residential properties in the data set did include heating.  To add a column with the same value for each row, we can write a statement that assigns that value to the new column in the DataFrame.  Similarly, we'll add a `County` column to indicate the source of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data['Heat'] = True\n",
    "home_data['County'] = \"Franklin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the first few rows to examine the state of our data before moving on to the next dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row"
   },
   "outputs": [],
   "source": [
    "home_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Errors: Licking County Auditor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can augment the Franklin County Auditor data with data from the [Licking County Auditor](https://www.lickingcountyohio.us/).  The data we'll use was obtained directly from the auditor's site and was not sampled or modified.  Let's try loading the data stored in `data/02-licking.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking = pd.read_csv(\"./data/02-licking.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception indicates that there was a problem parsing the data; specifically, pandas expected 33 fields but found 34 on line 3.  This could be due to pandas incorrectly guessing what the delimiter is.  We could use the csv module's *Sniffer* class to detect the delimiter but visual inspection will suffice.\n",
    "\n",
    "In the code below, we'll print the first five lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number = 0 \n",
    "with open(\"./data/02-licking.txt\") as infile:\n",
    "    while line_number < 5:\n",
    "        print(infile.readline())\n",
    "        line_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output, we can see that the delimiter is probably a semicolon rather than a comma.  The pandas *read_csv()* method takes a keyword argument, *delimiter*, that will allow us to specify the appropriate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking = pd.read_csv(\"./data/02-licking.txt\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception message indicates that pandas made it farther into the file before encountering an error.  On line 1608, it, pandas expected to find 181 fields based on the previous lines but instead found 182.  Let's investigate further.\n",
    "\n",
    "While we could iterate through the file and collect the line or lines that are of interest to use, we can use the *linecache* module to access a specific line within a file.  The code below extracts a typical line (one that did not cause a parser error) and the line that causes a problem.  After extracting the lines, the code displays their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "typical_line = linecache.getline(\"./data/02-licking.txt\", 2)\n",
    "error_line = linecache.getline(\"./data/02-licking.txt\", 1608)\n",
    "\n",
    "display(typical_line)\n",
    "display(error_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the lines in their entirety isn't very revealing.  \n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-8\"></a><mark> **Lab 8** A difference in the number of fields could be caused by a difference in the number of delimiters. In the cell below use the [*count()*](https://docs.python.org/3/library/stdtypes.html#str.count) method with each line to display the number of times the delimiter appears.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "It would be helpful if we could compare each fields values between the two lines.  To do this we'll use the String [*split()*](https://docs.python.org/3/library/stdtypes.html#str.split) method to separate each line into a list of field values.  In addition to the two lines we already have, we'll retrieve the first line from the data for column names.  We can use the itertools module's [zip_longest](https://docs.python.org/3/library/itertools.html#itertools.zip_longest) function to combine the list of values extracted from each line for comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "header_line = linecache.getline(\"./data/02-licking.txt\", 1)\n",
    "\n",
    "header_entries = header_line.split(\";\")\n",
    "typical_entries = typical_line.split(\";\")\n",
    "error_entries = error_line.split(\";\")\n",
    "\n",
    "for entry in zip_longest(header_entries, typical_entries, error_entries):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the values for the `fldTopo` header.  The \"typical line\" has no value whereas the \"error line\" has a value that seems related to the value associated with the previous field, `fldLUC`.  If we look back to the the display of each line's content, we can see that \"430 Resturant\" and \"cafeteria and/or bar\" are separated by a semicolon but should be kept together rather than split apart as different field values; note that \"Restaurant\" is misspelled in the source data.  The source data should use quoting if a delimiter appears as part of a data value or avoid using the delimiter in such a capacity.\n",
    "\n",
    "Now that we know what the problem is, there are a variety of ways to address the problem.  One way is to replace all instances of \"430 Restaurant; cafeteria and/or bar\" in the source text with something that doesn't have a semicolon prior to loading it in pandas.  In the code below, we assign the problematic value and its replacement value to variables.  After reading the content of the file, we use the *replace()* method to substitute occurrences of the first value with the second. We then load the data into pandas.  Because the pandas *read_csv()* function is expecting a file or stream, and not a string or bytes, we use the [*StringIO*](https://docs.python.org/3/library/io.html#io.StringIO) class to create a stream from the altered content. We specify \"python\" as the *engine* in the *read_csv()* method to avoid warnings about memory.  The Python CSV engine provided by pandas is more feature-complete but is slower than the default C engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "old_value = \"430 Resturant; cafteria and/or bar\"\n",
    "new_value = \"430 Resturant, cafeteria and/or bar\"\n",
    "\n",
    "with open(\"./data/02-licking.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "content = content.replace(old_value, new_value)\n",
    "    \n",
    "licking = pd.read_csv(io.StringIO(content), delimiter=\";\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this was relatively straightforward, there are disadvantages to this method.  The primary disadvantage here is that we iterate through the content of the file several times: first we read all the content, then we iterate through it to find and replace the problematic value, then iterate through it to load it into pandas; usually we only iterate through the file once when loading it into pandas.  While this is fine for relatively small files, we should avoid looping through the entirety of a file whenever possible.\n",
    "\n",
    "An alternative method would be to make use of pandas' support for [regular expressions](https://docs.python.org/3.2/library/re.html) when specifying the delimiter. We can use a [negative look-behind assertion](https://www.regular-expressions.info/lookaround.html) to indicate that a delimiter is any semicolon that isn't immediately preceded by the string \"Resturant\".  We could do this with the following call to *read_csv()*:\n",
    "\n",
    "```python\n",
    "licking = pd.read_csv(\"./data/02-licking.txt\", delimiter=\"(?<!Resturant);\", engine=\"python\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, let's display the first few lines to get sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Franklin country dataset, we'd like to filter this dataset for only residential buildings.  Unfortunately, there isn't documentation available to describe the content of each column so we'll have to do our best to infer meaning from the column name and values. Looking at the data above, it looks like `fldPropertyType` or `fldStyle` might be useful to determine which properties are residential and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.fldPropertyType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.fldStyle.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of style values are related to residential-type properties.  At this point, we might decide to choose specific styles to filter on or choose to simply exclude records without style information or those that correspond to a commercial style.  \n",
    "\n",
    "Let's see the styles associated with the *Dwelling* property type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking[licking.fldPropertyType == 'Dwelling'].fldStyle.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data to include only the *Dwelling* property didn't reduce the number of styles.  For this example, we'll filter the data to include only *Single Family*, *MFD Home*, *Tri-Level*, *Duplex*, *Bi-Level*, *Multi-Level*, *Condominum*, *Mobile Home*, *Triplex*, and *4-Level*.  Note that *Condominum* is misspelled in the source data.\n",
    "\n",
    "We can create a list of acceptable style values now that can be used to filter the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_styles = ['Single Family', 'MFD Home', 'Tri-Level', 'Duplex',\n",
    "                     'Bi-Level', 'Multi-Level', 'Condominum', 'Mobile Home',\n",
    "                     'Triplex', '4-Level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the other columns we'll need.  We had collected sales price data from the Franklin county dataset.  In this dataset, there are quite a few columns with \"sales\" in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in licking.columns if \"sales\" in column.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample data above, we will likely be interested in the collection of `fldSalesPrice` columns to determine the sales price. Let's look at the values of these columns for a small number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking[['fldSalesPrice1', 'fldSalesPrice2', 'fldSalesPrice3', 'fldSalesPrice4']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have nonzero, zero and `NaN` values.  In addition to these columns, the data also contains `fldSalesDate` columns. To get a better idea of what the prices represent, let's look at the date columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking[['fldSalesPrice1', 'fldSalesPrice2', 'fldSalesPrice3', 'fldSalesPrice4', \n",
    "         'fldSalesDate1', 'fldSalesDate2', 'fldSalesDate3', 'fldSalesDate4']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we move from the first price/date column to the second, the second price/date column to the third, and so on, we move backward in time.  It seems reasonable then that `fldSalesPrice1` represents the most recent sales price and the other columns are used to record historic sales data (if it exists).  We'll use the most recent sales price for our work so we'll only need `fldSalesPrice1`.\n",
    "\n",
    "Just as with the word \"sale\", there are a number of columns that contain the word \"area\". \n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-9\"></a><mark> **Lab 9** In the cell below, display all the columns with \"area\" in their name.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Here, we'll assume `fldFinishedLivingArea` contains the data need for area.\n",
    "\n",
    "Next, lets look for bathroom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in licking.columns if \"bath\" in column.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have columns corresponding to both full and half bathrooms as before but there is a third column for \"other\".  Let's see what values for this field look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.fldOtherBaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values themselves don't give a clear idea of what the field represents.  Given the lack of documentation, we'd likely contact the person or group responsible for the data for clarification; for our work here, we'll assume this field corresponds to quarter bathrooms.  \n",
    "\n",
    "Examining the sample data above, we can identify the other columns of interest.  Specifically, we'll extract the following columns from the Licking Country dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#home_columns = [\"AppraisedLand\", \"AppraisedBuilding\", \"LastSalePrice\", \"Area\", \"Rooms\", \"Bedrooms\", \"Bathrooms\", \"AirConditioning\", \"Heat\", \"Fireplaces\", \"YearBuilt\" ]\n",
    "licking_columns = [\"fldMarketLand\", \"fldMarketImprov\", \"fldSalesPrice1\", \"fldFinishedLivingArea\", \"fldRooms\", \"fldBedrooms\", \"fldFullBaths\", \"fldHalfBaths\", \"fldOtherBaths\",  \"fldHeating\", \"fldCooling\", \"fldFireplaceOpenings\", \"fldYearBuilt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a mask to filter the data based on style values.  Rather than compare one value to another as we did when filtering the Franklin Country data, we'll instead check if a values is among a list of values.  To do this, we can us the column's *isin()* method to test a value's membership in a specified list. \n",
    "\n",
    "Below we check if each value in the `fldStyle` column is in the `licking_styles` list we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.fldStyle.isin(licking_styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this mask in the usual way using bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset = licking[licking.fldStyle.isin(licking_styles)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the filtered data contains only the style values we had wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.fldStyle.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract only the columns we want. We use bracket notation again with the list of columns we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset = licking_subset[licking_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the first few rows of `licking_subset` to confirm we've extracted what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the various bathroom columns into one `Bathroom` column in the same way we combined them for the Franklin County dataset.\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-10\"></a><mark> **Lab 10** In the cell below, combine the values for full baths, half baths and other baths into one columns named `Bathrooom`.  Assume the value in `fldOtherBaths` is equivalent to a quarter of a full bathroom.  \n",
    "    \n",
    "Additionally, drop the original bathroom-related columns after computing the values for the new column\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Let's look at heating and cooling data . We extracted two columns from the original dataset `fldHeating` and `fldCooling` that contain heating and cooling data, respectively.  Let's look at the heating data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.fldHeating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target dataset doesn't differentiate among different data sources - it only indicates whether the property has heating or not.  For the Licking County data, we'd like to associate `False` with `No Heat` and `True` otherwise. We can do this by comparing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.fldHeating = licking_subset.fldHeating != \"No Heat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the value counts of the field to confirm that the number of `False` entries corresponds to the previous number of `No Heat` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.fldHeating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a name=\"Lab-11\"></a><mark> **Lab 11** In the cell below, replace the values in the `fldCooling` column with `True` to indicate that a property has cooling and `False` otherwise.\n",
    "\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "We can assume `fldFireplaceOpenings` correspond to fireplaces.  The only change we'll make is is to replace missing data with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.fldFireplaceOpenings.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be the last modification needed for the Licking County data.  In order to combine the `home_data` and `licking_subset` DataFrames, we need to make sure they have the same column names.  We'll rename columns in the same way we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.rename(\n",
    "    {'fldMarketLand': 'AppraisedLand',\n",
    "     'fldMarketImprov': 'AppraisedBuilding',\n",
    "     'fldSalesPrice1': 'SalePrice',\n",
    "     'fldFinishedLivingArea': 'Area', \n",
    "     'fldRooms':'Rooms',\n",
    "     'fldBedrooms':'Bedrooms',\n",
    "     'fldHeating': \"Heat\",\n",
    "     'fldCooling': 'AirConditioning', \n",
    "     'fldFireplaceOpenings': 'Fireplaces', \n",
    "     'fldYearBuilt': 'YearBuilt'\n",
    "    },\n",
    "    axis=1 ,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a column to the identify the source of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset['County'] = \"Licking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the columns in `home_data` and `licking_subset` are the same. Any differences will have to be corrected before merging the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how we can combine the DataFrames, let look at an example.  We'll start with two DataFrames, each wit columns `A` and `B` and with two rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "d2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('BA'))\n",
    "\n",
    "display(d1)\n",
    "display(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To append the content of one DataFrame to the end of another, we can use the DataFrame *append()* method.  We specify `ignore_index=True` to prevent duplication of index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.append(d2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *append()* method does not modify the original DataFrames directly but instead returns the combined DataFrame.  We can append the `licking_subset` DataFrame to `home_data` and assign the result to `home_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data = home_data.append(licking_subset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `home_data` now has data for two counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction from a GIS Dataset: Fairfield County Auditor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset we'll work with is the [Fairfield County Auditor Data](https://www.co.fairfield.oh.us/gis/).  This data is stored as [GIS](https://en.wikipedia.org/wiki/Geographic_information_system) data so loading it won't be as straightforward as reading a text file.  To access the data, we'll use the [GeoPandas](http://geopandas.org/) library, which will load the GIS data into a DataFrame so we can work with it in the same way we manipulated the other datasets. Recall that we installed the library using pip at the beginning of this notebook; with the libary installed, we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GIS data we're working with is stored in a [format](http://doc.arcgis.com/en/arcgis-online/reference/shapefiles.htm) specified by [ESRI](https://www.esri.com/en-us/home), the developer of [ArcGIS](https://www.arcgis.com/features/index.html), a popular GIS software product.  Data in this format is stored across several files and can be distributed as a single zip file.  We can load the data from the zip file using GeoPanda's *read_file()* function.  The data we'll be using is stored in `data/02-fairfield-gis.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.co.fairfield.oh.us/gis/\n",
    "fairfield = geopandas.read_file(\"zip://data/02-fairfield-gis.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by the *read_file()* method is a [GeoDataFrame](http://geopandas.org/data_structures.html#geodataframe), an extension of the pandas DataFrame with additional functionality.  The attributes and methods we've used with other DataFrames are available to use when working with GeoDataFrames.  For example, we can see the first few rows in the Fairfield County data using the *head()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, this looks like what we'd expect for county auditor data. The last column, however, is something we haven't seen yet.  The `geometry` column contains [data](http://desktop.arcgis.com/en/arcmap/10.3/analyze/arcpy-classes/geometry.htm) used to represent the location and shape of geometric features.  We can use this data to construct plots with map data.  \n",
    "\n",
    "In the code below, we use the [Matplotlib](https://matplotlib.org/) library to create a plot; we'll work with this library again later.  Because the geometric data represents geographic objects on Earth's surface, position information is stored using a [coordinate reference system](http://geopandas.org/projections.html).  For simpler manipulation, the code below converts data to use a reference system that relies on standard latitude and longitude which can be used in masks to filter the data.  Once filtered, the data is plotted.  In the resulting plot of [Lancaster](https://www.google.com/maps/place/Lancaster,+OH+43130/@39.7234464,-82.678719,12z/data=!3m1!4b1!4m5!3m4!1s0x88478a5e4f80f267:0x136dd5d79e3b4de5!8m2!3d39.7136754!4d-82.5993294), we can see features such as roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "fairfield = fairfield.to_crs({'init': 'epsg:4326'})\n",
    "fairfield[(fairfield.geometry.centroid.x >= -82.7) &\n",
    "          (fairfield.geometry.centroid.x <= -82.5) &\n",
    "          (fairfield.geometry.centroid.y >= 39.7) &\n",
    "          (fairfield.geometry.centroid.y <= 39.75)].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the task at hand, let's work on cleaning/filtering the Fairfield County data and merging it with the existing data.  \n",
    "\n",
    "Access to data that was used to construct the GIS dataset is available through a link on the Fairfield County Auditors site. The data is hosted on an external [site](http://downloads.ddti.net/fairfieldoh/) and includes a description of the database structure.  We can load the documentation in the notebook; the description for dwelling data is given on page 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"./data/02-fairfield-description.pdf#page=13\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the first few rows and the documentation, we might be able to filter the data based on the values in the `CLASS` column.  Let's look at its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.CLASS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume `R` represents residential data. We can see that we'll likely need the following columns as well.\n",
    "\n",
    "- `SFLA`: Living area\n",
    "- `YRBLT`: Year built\n",
    "- `RMTOT`: Total rooms\n",
    "- `RMBED`: Bedrooms\n",
    "- `FIXBATH`: Bathrooms\n",
    "- `FIXHALF`: Half-bathrooms\n",
    "- `HEAT`: Heat code\n",
    "- `PRICE`: Sales price\n",
    "- `APRLAND`: Appraised land value\n",
    "- `APRBLDG`: Appraised building value\n",
    "\n",
    "We can filter the data and extract the columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_columns = ['SFLA', 'YRBLT', 'RMTOT', 'RMBED', 'FIXBATH', \n",
    "                     'FIXHALF', 'HEAT', 'PRICE', 'APRLAND',  'APRBLDG']\n",
    "fairfield_subset = fairfield[fairfield.CLASS == 'R'][fairfield_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look the first few rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the `FIXBATH` and `FIXHALF` columns into a single column using the same method we used for the Franklin County data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset['Bathrooms'] = fairfield_subset.FIXBATH + 0.5 * fairfield_subset.FIXHALF\n",
    "fairfield_subset.drop([\"FIXBATH\", \"FIXHALF\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turing to the `HEAT` column, the data documentation indicates that the values in this column represent a \"heat code\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the tables in the database available online is one that defines these values.  The heat codes are as follows:\n",
    "\n",
    "- 1: None\n",
    "- 2: Basic\n",
    "- 3: Air conditioning\n",
    "- 4: Heat Pump\n",
    "\n",
    "We'll have to extract both heating and cooling data from this column. Notice that the output of *value_counts()* includes a row count for what appears to be missing data.  Before continuing, let's try to determine why there is a missing value.  To begin, let's use the *unique()* method for a better representation of the distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT.unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing value is an empty string.  At this point we need to decide if should assume that a missing value means no heat or some other type of heating that doesn't correspond to a code.  Let's look at a few rows where `HEAT` is an empty string.\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-12\"></a><mark> **Lab 12** Using a mask and the *head()* method, display the first five rows of `fairfield_subset` where the `HEAT` column has an empty string for a value.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "It appears that records where `HEAT` is an empty string, correspond to parcels with no living area.  At this point we can filter the data again to exclude records with an empty string in `HEAT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset = fairfield_subset[fairfield_subset.HEAT != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves the following values in the `HEAT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could write code that iterate through the rows of the DataFrame and sets heating and cooling values at the same time, it is easier to split this into two tasks: set the cooling value then set the heating value. \n",
    "\n",
    "We can create a new column, `AirConditioning` based on whether or not `HEAT` has a value of `'3'`. Because the column contains strings, it's important that our masks compare the column's values to another string rather than an iteger, i.e, our mask for air conditioning should be\n",
    "\n",
    "```python\n",
    "fairfield_subset.HEAT == '3'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rather than\n",
    "\n",
    "```python\n",
    "fairfield_subset.HEAT == 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset['AirConditioning'] = fairfield_subset.HEAT == '3'\n",
    "fairfield_subset.AirConditioning.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can assign a new value to `HEAT` based on the existing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT = fairfield_subset.HEAT != '1'\n",
    "fairfield_subset.HEAT.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps are to rename the columns, add data about the source, and append the Fairfield subset to our larger dataset.\n",
    "\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-13\"></a><mark> **Lab 13** In the cell below, rename the columns of the `fairfield_subset` DataFrame so they are consistent with the columns in `home_data`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "That leaves adding the county name and appending the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset['County'] = 'Fairfield'\n",
    "home_data = home_data.append(fairfield_subset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset contains data from three counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when we work with data, we encounter duplication - repetition of data.  We can see if `home_data` contains duplicate data by comparing the number of rows that would be left if we removed duplicates using the *drop_duplicates()* method to the number of rows in the current DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(home_data.drop_duplicates())/len(home_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that a little over 1% of our data corresponds to duplicates.  While the original data might not have contained duplicates, we created what appear to be duplicates by removing unneeded columns.  To see this more clearly, consider the following DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d1 = pd.DataFrame([[1, 2, 3], [1, 2, 4]], columns=list('ABC'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two rows of data are distinct.  However, if decide we no longer need column `C`, the rows will appear to be duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['C'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the percentage of rows that would be left after removing duplicates as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.drop_duplicates())/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the rows in `home_data` might have corresponded to distinct properties to begin with, at this point there is no way to distinguish between duplicated rows.  For now, however, we will leave the duplicates in the data knowing that they do represent different properties. \n",
    "\n",
    "Let's look at the data types of our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `Area` data is stored as objects.  We would probably like to store area as an integer or as a floating point value.  Let's try to convert the area values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.Area = home_data.Area.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception message indicates that some of the values contain commas. Before continuing, note the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "types = defaultdict(int)\n",
    "for value in home_data.Area:\n",
    "    value_type = type(value)\n",
    "    types[value_type] += 1\n",
    "\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Area` data contains some data stored as integers and other data stored as strings.  The number of strings corresponds to the number of entries from Licking country so the area data was likely stored with commas in that dataset.  To resolve this we can iterate through each row and, if the data is a string, remove any commas and convert to an integer. To iterate through the rows of a DataFrame we can use *iterrows()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in home_data.iterrows():\n",
    "    if isinstance(row.Area, str):\n",
    "        new_value = int(row.Area.replace(\",\", ''))\n",
    "        home_data.loc[index, 'Area'] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the DataFrame row by row can be slow and should generally be avoided.  An alternative approach is to create function that would handle one value at a time and use the DataFrame's [*apply()*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) method.\n",
    "\n",
    "Let's look at the data types for each column now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we loaded each data set, we checked some columns for missing values, represented by `NaN` by examining the value counts of those columns. We can check our `home_data` data frame for missing values we might have overlooked . In the code below, we first use the `isna()` method to return `True` for every value that is `NaN` and `False` otherwise.  We then calculate the sum of `True` values for each column using the `sum()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several columns with missing values.  For most of the columns, we might choose to remove any rows that contain missing information; for example, if we plan to use the data to see how different factors affect sales price, we probably don't want to keep records missing price data.  Before we start dropping rows, let's address the `Fireplaces` column - there are a significant number of missing values.  Recall that when we were working with the Fairfield data, we didn't have any fireplace data.  We can confirm that all the missing `Fireplaces` values correspond to records from Fairfield county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data[home_data.Fireplaces.isna()].County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first filter the dataset to consist of only those rows where there are missing fireplace values then calculate the value counts for the `County` column.  Indeed, the missing values are entirely from the `Fairfield` dataset.  What we do next is dependent on what we want to do with the data.  For now, we'll leave those missing values and, if we later try to determine a relationship between the number of fireplaces and sales price, we'll have to account for the fact that over 40,000 rows are missing fireplace data.\n",
    "\n",
    "For the other columns with missing values, we'll remove any rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the columns\n",
    "for column in home_data.columns:\n",
    "    # ignore the Fireplaces column\n",
    "    if column == \"Fireplaces\":\n",
    "        continue\n",
    "    # filter the dataframe to include non-NaN values for the column\n",
    "    home_data = home_data[home_data[column].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of missing values in each column shows that only `Fireplaces` is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `shape` property to confirm that we haven't accidentally delete most of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store the data in a SQLite database using SQLAlchemy and the DataFrame's [*to_sql()*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) method; the first argument to this method specifies the table name we'd like to use an the third argument indicates that we would like to replace any existing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///data/output.sqlite')\n",
    "home_data.to_sql(\"home_data\", con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Related Datasets\n",
    "\n",
    "In the previous examples, we worked on appending the rows of one DataFrame to another. Pandas supports a [variety of methods](https://pandas.pydata.org/pandas-docs/stable/merging.html) of combining data. Another common method is similar to a [database join](https://en.wikipedia.org/wiki/Join_%28SQL%29) where we combine combine the columns of two or more datasets.  Pandas DataFrames provide two methods that can be used for \"joins\": *join()* and *merge()*.  The *join()* method can be used when combining datasets based on index values and the *merge()* method can be used to combine datasets on any column values as well as index values; *merge()* is the more general method and *join()* ultimately relies on *merge()* to combine DataFrames.\n",
    "\n",
    "In the next example, we'll load two datasets related to vehicles. The first is [EPA/Department of Energy Data tracking](https://www.fueleconomy.gov/) the the fuel economy of vehicles and the second is [Norwegian vehicle sales data](https://www.kaggle.com/dmi3kno/newcarsalesnorway).  In a future less, we'll explore relationships within and between these datasets but we can combine the datasets first. We begin by loading the EPA data.\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-14\"></a><mark> **Lab 14** In the cell below, use the Pandas *read_csv()* function to load the data from `./data/02-vehicles.csv` an store it in a variable named `epa_data`. You might encounter a warning message about columns containing mixed types. One solution is to use the more feature-complete Python engine rather than the faster C engine; to to this, add `engine='python'` as an argument to *read_csv()*.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "An extract of the data description is presented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(filename=\"./data/02-vehicles-description.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we typically do after loading a new dataset, let's look at the first few rows to get a sense of the data contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore the data futher later.  For now, the important columns to consider will be `make`, `model`, and `year`.\n",
    "\n",
    "The next dataset contains data about Norwegian car sales and is stored using the [ISO 8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) encoding rather than standard ASCII or UTF-8; as such, we must specify the encoding as a parameter to the *read_csv()* function.  After loading it, we can examine the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"./data/02-vehicle-sales-norway.csv\", encoding=\"ISO-8859-1\")\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains year, make, and model information but the `Model` column is the combination of make and model.  In order to combine the datasets, we'll need to separate these two pieces of data.  To start, we'll rename the current `Model` column to `MakeModel`; this will allow us to preserve the existing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.rename({\"Model\": \"MakeModel\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the existing model data, it looks like the value that appears in the `Make` column is repeated as the beginning of the `MakeModel` column.  We use the `replace()` method to replace the `Make` value with an empty string - effectively removing it.  We will then use the `strip()` method to remove any leading or trailing white space.  \n",
    "\n",
    "We could iterate through the rows of the DataFrame using a for-loop; however, this tends to be slow. An alternative method is to use the DataFrame's *apply()* method that allows us to specify a function to [vectorize](https://en.wikipedia.org/wiki/Array_programming) an operation to an entire row or column.\n",
    "\n",
    "In the code below, we first define the function that we would like to apply.  The function is written as though it is applied one row at a time - pandas handles the vectorization for us.  To use the function to alter the data, we use the *apply ()* method and specify the name of the function and that we would like to apply it to each row by specifying `axis=1` which indicates that we would like to apply it along multiple columns and allow us to access their values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_make(row):\n",
    "    make = row['Make']\n",
    "    make_model = row['MakeModel']\n",
    "    # remove make and and leading/trailing white space\n",
    "    return make_model.replace(make, \"\").strip()\n",
    "    \n",
    "sales[\"Model\"] = sales.apply(remove_make, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that worked.  In order to join the two DataFrames, we'll specify the columns whose values will be compared for matching.  To simplify this, it is helpful to use the sames column names, including the same case, in both datasets. For our data, we can convert the columns in the sales data to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.rename({col: col.lower() for col in sales.columns},\n",
    "             axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge the data, we can use the *merge()* method of one of the two DataFrames.  When using *merge()*, we need to specify the other DataFrame and the columns used for matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_sales = epa_data.merge(sales, on=[\"year\", \"make\", \"model\"])\n",
    "epa_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a problem.  For some reason, pandas wasn't able to find any matches for a give (year, make, model) value in the `epa_data` set with the data in the `sales` dataset.  Let's look at a sample of value for each dataset. We'll use the `values` property to see how the data is stored rather than show the nicely formated representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_data[['year', 'make', 'model']].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[['year', 'make', 'model']].head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the values in the `make` column in the `sales` data have some trailing white space whereas the values in the `epa_data` do not.  This difference is enough to prevent pandas from matching the values.  To address this, we can create a function that strips white space from values.  To be safe, we can also convert the values to lowercase to ensure differences in case don't cause problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip white space and convert to lower case\n",
    "def strip_lower(x):\n",
    "    return x.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this function to the `remove_make()` function we defined above.  This function operates on an individual value rather than an entire row.  To apply this to the values in a column of a DataFrame, we should use the *apply()* method associated with the column itself rather than the DataFrame.  The following code applies the function to `make` and `model` columns of both DataFrames; the `year` data in both DataFrames are stored as integers and do not require this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_data.make = epa_data.make.apply(strip_lower)\n",
    "epa_data.model = epa_data.model.apply(strip_lower)\n",
    "\n",
    "sales.make = sales.make.apply(strip_lower)\n",
    "sales.model = sales.model.apply(strip_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values in the DataFrames again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_data[['year', 'make', 'model']].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[['year', 'make', 'model']].head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the white space has been removed and the make and model data are lowercased.  Let's try merging the DataFrames again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_sales = epa_data.merge(sales, on=[\"year\", \"make\", \"model\"])\n",
    "epa_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the merge was successful.  We can use the DataFrame's *shape* property to see the number of columns and rows it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `epa_sales` DataFrame contains 1562 rows and 87 columns. We don't need to keep all these columns so let's select a subset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_sales = epa_sales[['city08', 'co2', 'comb08', 'cylinders',\n",
    "                       'displ', 'fuelType1', 'highway08', \n",
    "                       'make','model','VClass','year', 'quantity']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this DataFrame to the database use created previously so we can access the data later.\n",
    "\n",
    "<hr>\n",
    "<a name=\"Lab-15\"></a><mark> **Lab 15** In the cell below, use the `epa_sales` DataFrame's *to_sql()* method to save the data in the same database that we stored property information. Use the table name `epa_sales`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  ```python\n",
    "    franklin.PROPTYP.unique()\n",
    "    ``` \n",
    "\n",
    "    or \n",
    "\n",
    "    ```python\n",
    "    franklin.PROPTYP.value_counts()\n",
    "    ```\n",
    "    \n",
    "2. ```python\n",
    "   len(franklin_subset) < len(franklin)\n",
    "   ```\n",
    "   \n",
    "3. ```python\n",
    "   franklin_subset['APPRLND'] = franklin_subset.APPRLND.astype(int)\n",
    "   franklin_subset = franklin_subset[franklin_subset.APPRLND > 0]\n",
    "   ```\n",
    "   \n",
    "4. ```python\n",
    "   franklin_subset[['BATHS', 'HBATHS']].dtypes\n",
    "   ```\n",
    "   \n",
    "5. ```python\n",
    "   franklin_subset.drop(['BATHS', \"HBATHS\"], axis=1, inplace=True)\n",
    "   ```\n",
    "   \n",
    "6. ```python\n",
    "   franklin_subset.FIREPLC = franklin_subset.FIREPLC.fillna(value=0)\n",
    "   ```\n",
    "   \n",
    "   or\n",
    "   \n",
    "   ```python\n",
    "   franklin_subset.FIREPLC.fillna(value=0, inplace=True)\n",
    "   ```\n",
    "   \n",
    "7. ```python\n",
    "   home_data.columns\n",
    "   ```\n",
    "   \n",
    "8. ```python\n",
    "   display(typical_line.count(\";\"))\n",
    "   display(error_line.count(\";\"))\n",
    "   ```\n",
    "   \n",
    "9. ```python\n",
    "   for column in licking.columns:\n",
    "       if \"area\" in column.lower():\n",
    "           display(column)\n",
    "   ```\n",
    "   \n",
    "10. ```python\n",
    "    licking_subset['Bathrooms'] = (licking_subset.fldFullBaths.fillna(0) + \n",
    "                                   0.5 * licking_subset.fldHalfBaths.fillna(0) + \n",
    "                                   0.25 * licking_subset.fldOtherBaths.fillna(0))\n",
    "   licking_subset.drop([\"fldFullBaths\", \"fldHalfBaths\", \"fldOtherBaths\"], axis=1, inplace=True)\n",
    "    ```\n",
    "    \n",
    "11. ```python\n",
    "    licking_subset.fldCooling = licking_subset.fldCooling == \"Central\"\n",
    "    ```\n",
    "    \n",
    "12. ```python\n",
    "    fairfield_subset[fairfield_subset.HEAT == ''].head()\n",
    "    ```\n",
    "    \n",
    "13. ```python\n",
    "    fairfield_subset.rename(\n",
    "        {'APRLAND': 'AppraisedLand',\n",
    "         'APRBLDG': 'AppraisedBuilding',\n",
    "         'PRICE': 'SalePrice',\n",
    "         'SFLA': 'Area', \n",
    "         'RMTOT':'Rooms',\n",
    "         'RMBED':'Bedrooms',\n",
    "         'HEAT': 'Heat',\n",
    "         'YRBLT': 'YearBuilt'\n",
    "        },\n",
    "        axis=1 ,\n",
    "        inplace=True\n",
    "    )\n",
    "    ```\n",
    "\n",
    "14. ```python\n",
    "    epa_data = pd.read_csv(\"./data/02-vehicles.csv\")\n",
    "    ```\n",
    "    \n",
    "    or \n",
    "    \n",
    "    ```python\n",
    "    epa_data = pd.read_csv(\"./data/02-vehicles.csv\", engine=\"python\")\n",
    "    ```\n",
    "\n",
    "15. ```python\n",
    "    epa_sales.to_sql(\"epa_sales\", con=engine, if_exists='replace')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loading and cleansed the data to some extent, next step might be to being exploring the data. In the next unit, we'll calculate simple, descriptive statistics and create exploratory visualizations using the data we prepared in this this unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [GeoPandas Documentation](http://geopandas.org/)\n",
    "- [*Data Cleaning: Problems and Current Approaches* by Rahm and Do](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.8661&rep=rep1&type=pdf)\n",
    "- [*Data Mining: Concepts and Techniques* by Han, Pei, and Kamber, Section 3.2: Data Cleansing (Safari Books)](http://proquest.safaribooksonline.com.cscc.ohionet.org/book/databases/data-warehouses/9780123814791/3dot-data-preprocessing/32_data_cleaning?uicode=ohlink)\n",
    "- [*Python Data Science Handbook* by VanderPlas, Chapter 3: Data Manipulation with Pandas](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html)\n",
    "- [*Python for Data Analysis* by Wes McKinney, Chapter 7: Data Cleaning and Preparation (Safari Books)](http://proquest.safaribooksonline.com.cscc.ohionet.org/book/programming/python/9781491957653/data-cleaning-and-preparation/data_preparation_html?uicode=ohlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to compete each exercise below.  Add cells as necessary. \n",
    "\n",
    "1. Road data, maintained by the State of Ohio, for Franklin County is available in `./data/02-roads.csv` with a description of columns in `./data/02-roads-description.csv`.  Load the data and perform the following tasks.\n",
    "\n",
    "    - Filter the data to include only roads with speed limits of 55 mph or greater\n",
    "    - Drop any columns missing data for every row in the filtered data \n",
    "    - Drop any rows missing data in the filtered data\n",
    "    - Display the first few rows of filtered data\n",
    "    \n",
    "2. In the previous unit, we loaded data from two tables by performing a `JOIN` as part of a SQL Query using the following code.\n",
    "\n",
    "    ```python\n",
    "    from sqlalchemy import create_engine\n",
    "    engine = create_engine('sqlite:///data/01-chinook.sqlite')\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM Invoice INNER JOIN Customer \n",
    "    ON Invoice.CustomerId = Customer.CustomerId \n",
    "    ORDER BY Invoice.InvoiceID \n",
    "    \"\"\"\n",
    "\n",
    "    invoice_customer = pd.read_sql_query(query, engine)\n",
    "    invoice_customer.head(5)\n",
    "    ```\n",
    "    Use pandas to retrieve the data from the `Invoice` and `Customer` tables as two separate DataFrames then use `merge()` to create a new DataFrame that joins the data.  Compare the result of using *merge()* to using `JOIN` in the SQL Query. Display the first few rows of the merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
